__authors__ = "Srinivas Veerla, Guyuan TANG"
__copyright__ = "Copyright 2024, Srinivas Veerla"
__email__ = "srinivas.veerla@med.lu.se"
__license__ = "GPL-3"

import pandas as pd

#Configuration file
config="config/config.yaml"

#Read sample information file
sample_info = (pd.read_csv(config['sample_info'],dtype={'sample_name':str,'path':str}).set_index('sample_name', drop=False))

#Working directory location
work_dir = config['work_dir']

#Result data files storage location
results = config['output_dir']

########### 1 sWGS raw data preprocessing ##########
rule preprocess:
	input:
		results + '01_preprocess/html/' + sample_group + '_multiqc_report.html'

rule fastp:
	input:
      R1 = lambda wildcards: sample_info.loc[wildcards.sample, 'fastq_1'],
      R2 = lambda wildcards: sample_info.loc[wildcards.sample, 'fastq_2']
  output:
  	R1 = results + '01_preprocess/reads/{sample}_R1_preprocess.fastq.gz',
  	html = results + '01_preprocess/html/{sample}_fastp.html',
  	R2 = results + '01_preprocess/reads/{sample}_R2_preprocess.fastq.gz' 
  log: 'log/fastp/{sample}_fastp.log'
  threads: 20
  params: json = results + '01_preprocess/html/{sample}_fastp.json'
  conda: "envs/preprocess_env.yaml"
  shell: """
    fastp --detect_adapter_for_pe \
    --correction --cut_right --thread {threads} \
    --html {output.html} --json {params.json} \
    --in1 {input.R1} --in2 {input.R2} \
    --out1 {output.R1} --out2 {output.R2} \
    2>{log}
    
    rm {params.json}
  """
 
 # 1.2 quality assessment of preprocessed reads with fastqc
rule fastqc:
    ### we will use fastqc to generate the quality control stats from the outputs of fastp
  input:
    R1_seq = results + '01_preprocess/reads/{sample}_R1_preprocess.fastq.gz',
    R2_seq = results + '01_preprocess/reads/{sample}_R2_preprocess.fastq.gz'
  output:
    R1_html = results + '01_preprocess/html/{sample}_R1_preprocess_fastqc.html',
    R1_qc = results + '01_preprocess/reports/{sample}_R1_preprocess_fastqc.zip',
    R2_html = results + '01_preprocess/html/{sample}_R2_preprocess_fastqc.html',
    R2_qc = results + '01_preprocess/reports/{sample}_R2_preprocess_fastqc.zip'
  log: 'log/fastqc/{sample}.fastqc.log'
  params: 
    outdir = results + '01_preprocess/reports/',
    out_html = results + '01_preprocess/html/'
  threads: 20
    #conda: 'envs/preprocess_env.yaml'
  shell: """
    fastqc -o {params.outdir} {input.R1_seq} {input.R2_seq} 2>{log}
    mv {params.outdir}*_fastqc.html {params.out_html}
    """

# 1.3 quality assessment report for the reads
rule multiqc:
    ### we will use multiqc here to generate reports from the output of fastqc.
    input:
        R1_qc = expand(results + '01_preprocess/reports/{sample}_R1_preprocess_fastqc.zip', sample=sample_df.sample_name),
        R2_qc= expand(results + '01_preprocess/reports/{sample}_R2_preprocess_fastqc.zip', sample=sample_df.sample_name)
    output:
        results + '01_preprocess/html/' + sample_group + '_multiqc_report.html'
    log:
        'log/multiqc.log'
    conda: 'envs/multiqc_env.yaml'
    params: 
        out_name = sample_group + '_multiqc_report.html',
        indir = results + '01_preprocess/reports',
        outdir = results + '01_preprocess/html/',
        group = sample_group
    shell: """
    multiqc -f -n {params.out_name} \
    -o {params.outdir} {params.indir} >{log} 2>{log}
    rm -r {params.outdir}/{params.group}_multiqc_report_data/
    """

########## 2 Alignment ####################
"""
The final output for the alignment step would be the unsorted BAM files for all the included samples.
"""
rule alignment:
    input:
        expand(results + '02_alignment/{sample}.unsorted.sam', sample=sample_df.sample_name)

# 2.1 downloading the human reference genome (hg38)

#rule download_hg38:
#    output:
#        genome = 'resources/genome/hg38.ref.fa.gz'
#    log:
#        'log/genome/download_hg38.log'
#    shell: """
#    for i in $(seq 1 22) X; do echo $i; wget http://hgdownload.cse.ucsc.edu/goldenPath/hg38/chromosomes/chr${i}.fa.gz -O resources/genome/chr${i}.fa.gz; done 2>{log}
#
#    gunzip resources/genome/*.gz
#    
#    for a in $(seq 1 22) X; do cat resources/genome/chr${a}.fa >> resources/genome/hg38.ref.fa; done
#
#    gzip resources/genome/hg19.ref.fa
#
#    rm *.fa
#    """

# 2.2 indexing the hg38 reference genome
#rule bwa_index:
#    ### use bwa to index the reference genome
#    input:
#        genome = 'resources/genome/hg38.ref.fa.gz'
#    output:
#        multiext('resources/genome/hg38', ".amb", ".ann", ".bwt", ".pac", ".sa")
#    conda: 'envs/alignment.yaml'
#    log: 'log/bwa/bwa_index.log'
#    params: outdir = 'resources/genome/'
#    threads: 10
#    shell: """
#    bwa index -p hg38 -a bwtsw {input.genome} 2>{log}
#    mv hg38.* {params.outdir}
#    """

# 2.3 mapping the reads to the indexed reference genome
rule map_reads:
    ### use bwa again for alignment
    input: 
        idx = rules.bwa_index.output,
        link_up = rules.preprocess.input,
        R1 = results + '01_preprocess/reads/{sample}_R1_preprocess.fastq.gz',
        R2 = results + '01_preprocess/reads/{sample}_R2_preprocess.fastq.gz'
    output:
        results + '02_alignment/{sample}.unsorted.sam'
    log: 'log/bwa_mapping/{sample}.log'
    params:
        index_ref = 'resources/genome/hg38'
    #conda: 'envs/alignment.yaml'
    threads: config['bwa_mapping']['threads']
    shell: """
    bwa mem -M -t {threads} \
        {params.index_ref} {input.R1} {input.R2} > {output} \
        2>{log}
    """

########## 3 Clean-up ####################
"""
The final output for the clean-up step should be the sorted, marked, and indexed BAM files.
"""
rule clean_up:
    input: 
        expand(results + '03_clean_up/{sample}/{sample}.sorted.dedup.bai', sample=sample_info.sample_name)

# 3.1 sorting the SAM files
rule sort_sam: 
    ### using Picard to sort the sam files 
    input:
        sam = results + '02_alignment/{sample}.unsorted.sam',
        link_up = rules.alignment.input
    output:
        results + '03_clean_up/{sample}/{sample}.sorted.sam'
    log: 'log/sort_sam/{sample}.log'
    conda: 'envs/clean_up.yaml'
    shell: """
    picard SortSam \
        INPUT={input.sam} \
        OUTPUT={output} \
        SORT_ORDER=coordinate \
        2>{log}

    """

# 3.2 marking dupicates and de-duplication
rule de_duplicate:
    ### using Picard to remove PCR duplicates, and convert SAM file into BAM files
    input: 
        results + '03_clean_up/{sample}/{sample}.sorted.sam'
    output:
        dedup_bam = results + '03_clean_up/{sample}/{sample}.sorted.dedup.bam',
        sort_bam = results + '03_clean_up/{sample}/{sample}.sorted.bam'
    log: 'log/de_duplicate/{sample}.log'
    threads:10
    params:
        metrix_file = results + '03_clean_up/{sample}/{sample}.metrics.txt'
    conda: 'envs/clean_up.yaml'
    shell: """

    picard MarkDuplicates \
        INPUT={input} \
        OUTPUT={output.dedup_bam} \
        METRICS_FILE={params.metrix_file} \
        REMOVE_DUPLICATES=true \
        ASSUME_SORT_ORDER=coordinate \
        CLEAR_DT=false \
        2>{log}
    
    samtools view -bo {output.sort_bam} {input}
    qualimap bamqc -bam {output.sort_bam} --java-mem-size=4G

    """


# 3.3 indexing the BAM files
rule index_bam:
    ### using samtools to show the stats of the sorted and deduplicates outputs and to index the bam files
    input:
        results + '03_clean_up/{sample}/{sample}.sorted.dedup.bam'
    output:
        results + '03_clean_up/{sample}/{sample}.sorted.dedup.bai'
    log: 'log/bam_stat/' + sample_group + '/{sample}.log'
    threads: 20
    conda: 'envs/clean_up.yaml'
    shell: """
    samtools flagstat {input} | tee {log}

    samtools index -@ {threads} -o {output} {input}

    qualimap bamqc -bam {input} --java-mem-size=4G

    """ 
    
